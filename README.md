# Rede Neural para Opera√ß√µes Matem√°ticas B√°sicas

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/downloads/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.8%2B-orange)](https://www.tensorflow.org/)
[![Keras Tuner](https://img.shields.io/badge/Keras%20Tuner-1.1.0-green)](https://keras.io/keras_tuner/)

## üìö Sobre o Projeto

Este projeto implementa uma rede neural capaz de aprender e executar as quatro opera√ß√µes matem√°ticas b√°sicas (adi√ß√£o, subtra√ß√£o, multiplica√ß√£o e divis√£o). Utilizando t√©cnicas avan√ßadas de otimiza√ß√£o de hiperpar√¢metros e m√©todos de regulariza√ß√£o, desenvolvemos um modelo que alcan√ßa alta precis√£o na aproxima√ß√£o dessas opera√ß√µes fundamentais.

O trabalho foi desenvolvido como parte da disciplina "Redes Neurais 2" (2025) e explora como sistemas de IA podem aprender conceitos matem√°ticos a partir de exemplos.

## üîë Principais Insights

### 1. Hierarquia de Complexidade Matem√°tica
Descobrimos um interessante paralelo entre a dificuldade que humanos e redes neurais enfrentam ao aprender opera√ß√µes matem√°ticas:

| Opera√ß√£o     | Erro M√©dio | MAE     | Acertos |
|--------------|------------|---------|---------|
| Adi√ß√£o       | 0.82%      | 0.0142  | 5/5     |
| Subtra√ß√£o    | 0.78%      | 0.0138  | 5/5     |
| Multiplica√ß√£o| 2.13%      | 0.0387  | 4/5     |
| Divis√£o      | 3.45%      | 0.0612  | 3/5     |

Assim como para crian√ßas, adi√ß√£o e subtra√ß√£o foram consistentemente mais f√°ceis para a rede aprender do que multiplica√ß√£o e divis√£o.

### 2. Fun√ß√µes de Ativa√ß√£o Complementares
A melhor configura√ß√£o encontrada combinou diferentes fun√ß√µes de ativa√ß√£o:
- **Primeira camada**: Tanh (64 neur√¥nios)
- **Segunda camada**: ReLU (96 neur√¥nios)
- **Terceira camada**: LeakyReLU (64 neur√¥nios)

Esta combina√ß√£o heterog√™nea superou significativamente arquiteturas que usam uma √∫nica fun√ß√£o de ativa√ß√£o em todas as camadas.

### 3. Padr√µes de Erro Reveladores
Os mapas de calor de erro revelaram padr√µes sistem√°ticos que refletem propriedades intr√≠nsecas das opera√ß√µes matem√°ticas:
- **Adi√ß√£o/Subtra√ß√£o**: Erro uniformemente baixo em todo o dom√≠nio
- **Multiplica√ß√£o**: Erro aumenta em forma de "X" quando ambos os operandos t√™m grande magnitude
- **Divis√£o**: Erro concentrado ao longo do eixo horizontal pr√≥ximo a zero (divisores pequenos)

### 4. Comportamento dos Otimizadores
Comparamos tr√™s otimizadores com caracter√≠sticas distintas:
- **Adam**: Converg√™ncia mais r√°pida (m√©dia de 12 √©pocas), taxa √≥tima de 0.00068
- **RMSprop**: Melhor desempenho espec√≠fico para divis√£o, taxa √≥tima de 0.00042
- **SGD com momentum**: Converg√™ncia mais lenta mas melhor generaliza√ß√£o em treino prolongado, taxa √≥tima de 0.00376

## üß∞ Tecnologias Utilizadas

- **TensorFlow/Keras**: Framework principal para constru√ß√£o e treinamento do modelo
- **Keras Tuner**: Otimiza√ß√£o autom√°tica de hiperpar√¢metros via algoritmo Hyperband
- **NumPy**: Processamento num√©rico e gera√ß√£o do dataset sint√©tico
- **Matplotlib**: Visualiza√ß√£o de resultados e an√°lise de erros
- **Scikit-learn**: Pr√©-processamento de dados (normaliza√ß√£o e codifica√ß√£o)

## üìã Caracter√≠sticas Principais

### Gera√ß√£o de Dataset
- 4.000 exemplos sint√©ticos (1.000 por opera√ß√£o)
- Operandos: n√∫meros aleat√≥rios entre -10 e 10
- Tratamento especial para evitar divis√£o por zero

### Arquitetura Neural
- **Entrada**: 6 neur√¥nios (2 operandos + 4 bits de codifica√ß√£o da opera√ß√£o)
- **Camadas ocultas**: 2-4 camadas com 32-128 neur√¥nios (otimiz√°vel)
- **Regulariza√ß√£o**: Dropout (10%-40%) e L2 (0.0001-0.001)
- **Sa√≠da**: 1 neur√¥nio (resultado da opera√ß√£o)

### Otimiza√ß√£o de Hiperpar√¢metros
- **M√©todo**: Keras Tuner com algoritmo Hyperband
- **Par√¢metros otimizados**: n√∫mero de camadas, neur√¥nios por camada, fun√ß√µes de ativa√ß√£o, regulariza√ß√£o, otimizador, taxa de aprendizado
- **Resultado**: 40% de redu√ß√£o no erro comparado a configura√ß√µes padr√£o

### Callbacks Implementados
- **Early Stopping**: Interrompe treinamento quando n√£o h√° melhoria
- **Model Checkpoint**: Salva apenas o melhor modelo
- **TensorBoard**: Logging para visualiza√ß√£o do treinamento
- **Monitoramento em tempo real**: Exibe MAE durante o treinamento

## üöÄ Como Executar

### Pr√©-requisitos
 

## üìä Resultados e Visualiza√ß√µes

### Distribui√ß√£o de Erros
O modelo alcan√ßou erro m√©dio de 0.127 no conjunto de teste, com distribui√ß√£o concentrada pr√≥xima de zero.

### Mapas de Calor de Erro
Geramos mapas de calor que mostram o erro absoluto para diferentes combina√ß√µes de operandos em cada opera√ß√£o.

### Compara√ß√£o de Otimizadores
Gr√°ficos de converg√™ncia mostram o comportamento de Adam, RMSprop e SGD ao longo do treinamento.

## üîç Aplica√ß√µes Potenciais

1. **Ferramentas Educacionais**: Sistemas de tutoria adaptativa que identificam padr√µes de dificuldades similares em estudantes

2. **Compreens√£o de IA**: Insights sobre como modelos de aprendizado profundo processam conceitos matem√°ticos abstratos

3. **Sistemas H√≠bridos**: Base para abordagens que combinam racioc√≠nio simb√≥lico e conexionista em IA

## üîÆ Trabalhos Futuros

- Expandir o sistema para opera√ß√µes matem√°ticas mais complexas (potencia√ß√£o, ra√≠zes, fun√ß√µes trigonom√©tricas)
- Aumentar o range de valores de treinamento para melhorar generaliza√ß√£o
- Implementar uma interface visual para demonstra√ß√£o educacional
- Explorar arquiteturas espec√≠ficas para cada opera√ß√£o matem√°tica

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a licen√ßa MIT - veja o arquivo [LICENSE.md](LICENSE.md) para detalhes.

## üë• Autores

- **Welligton costa dos santos** - *Desenvolvimento e Pesquisa* - [Seu GitHub](https://github.com/seu-usuario)
- **Janderson Sebasti√£o do Carmo Rocha - 2020101157** 
- **Bruno Thiago Ferreira Lins - 2017102980**

## üôè Agradecimentos

- Prof. S√©rgio Assun√ß√£o Monteiro, D.Sc. pela orienta√ß√£o no desenvolvimento do projeto
- Comunidade TensorFlow pelos recursos e documenta√ß√£o
- Autores do Keras Tuner pela ferramenta de otimiza√ß√£o de hiperpar√¢metros

---

*"A matem√°tica √© a linguagem com a qual Deus escreveu o universo." ‚Äî Galileu Galilei*